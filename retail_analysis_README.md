# Retail Sales Data Analysis Tools

This repository contains tools for analyzing the MCCS Retail Sales data, focusing on identifying trends and patterns in the 1.5GB dataset.

## Overview

The analysis tools consist of:

1. **Static Analysis Script** (`retail_trends_analysis.py`): A comprehensive Python script that processes the entire dataset and generates static visualizations and a summary report.

2. **Interactive Dashboard** (`retail_trends_dashboard.py`): A Streamlit-based interactive dashboard that allows for dynamic exploration of the data with filtering capabilities.

3. **AI-Powered Insights** (`retail_llm_insights.py`): A module that uses OpenAI's language models to generate advanced insights and recommendations from the analysis results.

## Quick Start

Run the interactive analysis tool with a single command:

```bash
./run_retail_analysis.sh
```

Or run specific analysis options directly with these one-liners:

For static analysis:
```bash
bash -c "python -m venv venv && source venv/bin/activate && pip install -r retail_analysis_requirements.txt && python retail_trends_analysis.py"
```

For interactive dashboard:
```bash
bash -c "python -m venv venv && source venv/bin/activate && pip install -r retail_analysis_requirements.txt && streamlit run retail_trends_dashboard.py"
```

For AI insights (requires API key):
```bash
bash -c "python -m venv venv && source venv/bin/activate && pip install -r retail_analysis_requirements.txt && python retail_llm_insights.py --api_key YOUR_API_KEY"
```

## Data Description

The dataset contains detailed retail sales data from Marine Corps Community Services stores for Dec 2024 - Jan 2025:

- **Size**: ~1.5GB (10,143,887 rows)
- **Format**: Available as both Parquet (`data/rawdata/MCCS_RetailData.parquet`) and CSV (`data/convertedcsv/MCCS_RetailData.csv`)
- **Structure**: Each row represents a transaction line item on a retail receipt

### Key Columns:

- `SALE_DATE` / `SALE_DATE_TIME`: Date and time of the transaction
- `STORE_FORMAT`: Store type (Main Store / Marine Mart)
- `COMMAND_NAME`: Name of Marine Corps base/installation
- `SITE_ID` / `SITE_NAME`: Store identification and name
- `SLIP_NO`: Receipt/transaction number
- `LINE`: Line number within the receipt
- `ITEM_ID` / `ITEM_DESC`: Product identification and description
- `EXTENSION_AMOUNT`: Net price for all units purchased
- `QTY`: Quantity sold
- `RETURN_IND`: Return indicator (Y/N)
- `PRICE_STATUS`: Price status (R=Regular, P=Promotion, M=Markdown)

## Requirements

To run these analysis tools, you'll need:

```
pandas
numpy
matplotlib
seaborn
dask
plotly
streamlit
langchain
langchain-openai
openai
```

You can install all dependencies with:

```bash
pip install -r retail_analysis_requirements.txt
```

## Usage

### Static Analysis Script

The static analysis script processes the entire dataset and generates visualizations and a summary report in the `retail_analysis_results` directory.

```bash
python retail_trends_analysis.py
```

This will:
1. Load and process the full dataset using Dask for memory efficiency
2. Generate visualizations for temporal trends, product analysis, store performance, etc.
3. Create a summary report with key insights

**Note**: This script may take several minutes to run due to the large dataset size.

### Interactive Dashboard

The Streamlit dashboard provides an interactive way to explore the data with filtering capabilities.

```bash
streamlit run retail_trends_dashboard.py
```

This will:
1. Launch a web browser with the interactive dashboard
2. Allow filtering by date range, store format, command, and price status
3. Display dynamic visualizations that update based on the selected filters
4. Provide access to AI-powered insights through the "AI Insights" section

**Note**: For performance reasons, the dashboard uses a 10% sample of the data by default. This can be adjusted in the code if needed.

### AI-Powered Insights

The LLM insights module can be used to generate advanced insights from the analysis results:

```bash
python retail_llm_insights.py --api_key "your-openai-api-key" [--question "your specific question"]
```

This will:
1. Read the summary report generated by the static analysis
2. Send the data to OpenAI's language models
3. Generate comprehensive insights and recommendations
4. Save the insights to a markdown file in the results directory

You can also access this functionality through:
- The interactive dashboard's "AI Insights" section
- The run script's menu option 3

## Analysis Areas

The tools focus on the following analysis areas:

1. **Temporal Trends**
   - Daily/weekly sales patterns
   - Day of week analysis
   - Hour of day analysis

2. **Product Analysis**
   - Top-selling products by revenue and quantity
   - Price status impact (Regular/Promotion/Markdown)

3. **Store Performance**
   - Sales by store format
   - Performance by command
   - Top performing stores

4. **Transaction Analysis**
   - Transaction size distribution
   - Transaction value distribution
   - Return patterns

5. **AI-Generated Insights**
   - Overall performance assessment
   - Key strengths and weaknesses identification
   - Trend and pattern recognition
   - Strategic recommendations
   - Suggested areas for further analysis

## Output

### Static Analysis

The static analysis script generates:
- PNG visualizations in the `retail_analysis_results` directory
- A summary report (`summary_report.txt`) with key metrics and insights

### Interactive Dashboard

The dashboard provides:
- Real-time filtering and visualization
- Key metrics that update based on filters
- The ability to explore the raw data
- Access to AI-powered insights

### AI Insights

The LLM insights module generates:
- A markdown file (`llm_insights.md`) with comprehensive analysis
- Strategic recommendations based on the data
- Answers to specific questions about the retail data

## Performance Considerations

Given the large size of the dataset (1.5GB):

- The static analysis script uses Dask for memory-efficient processing
- The interactive dashboard uses a sample of the data for responsiveness
- The dashboard caches the sampled data as a parquet file for faster subsequent loads

## Extending the Analysis

To extend these tools:

1. Add new visualizations to `retail_trends_analysis.py`
2. Add new interactive components to `retail_trends_dashboard.py`
3. Modify the data processing to extract additional insights
4. Customize the prompts in `retail_llm_insights.py` to focus on specific business questions

## Troubleshooting

If you encounter memory issues:
- For the static script: Adjust the Dask blocksize parameter
- For the dashboard: Reduce the sample fraction in the `load_data()` function
